# =============================================================================
# DocBases v2.0 - Environment Configuration
# =============================================================================
# SETUP INSTRUCTIONS:
#   1. Copy this file: cp .env.example .env
#   2. Edit .env and fill in your values
#   3. Never commit .env with real API keys to version control!
#
# PRIORITY ORDER (highest to lowest):
#   1. CLI arguments        (e.g., --model gpt-4o)
#   2. Environment variables (from .env file)
#   3. User config          (~/.docbases/config.yaml)
#   4. Project config       (.docbases/config.yaml)
#   5. Default values
#
# DOCUMENTATION: https://github.com/yourusername/doc-bases#configuration
# =============================================================================

# =============================================================================
# 1. LLM PROVIDER & MODEL CONFIGURATION
# =============================================================================
# Choose your LLM provider. Supported options:
#   - openai   : OpenAI GPT models (requires OPENAI_API_KEY)
#   - google   : Google Gemini models (requires GOOGLE_API_KEY)
#   - groq     : Groq Cloud (requires GROQ_API_KEY)
#   - ollama   : Local models via Ollama (requires LLM_API_BASE)
# Default: ollama
# Recommended: Start with 'ollama' for local, free inference
# Cloud-based: Use 'openai' for best compatibility and performance
# Speed-focused: Use 'groq' for fastest response times
# Budget-friendly: Use 'google' with gemini-1.5-flash for cost savings
# Type: string
# ENV: LLM_PROVIDER
# CLI: --provider <name>, -p <name>
# =============================================================================
LLM_PROVIDER=ollama

# Specific LLM model to use with the provider above.
# Recommendation: Start with llama3.1:8b for Ollama (balanced, 8GB RAM)
#
# âœ“ OPENAI MODELS (https://platform.openai.com/docs/models)
#   Production-grade: gpt-4o, gpt-4o-mini, gpt-4-turbo
#   Legacy:           gpt-3.5-turbo
#
# âœ“ GOOGLE GEMINI MODELS (https://ai.google.dev/models)
#   Latest:  gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash
#   Speed:   gemini-1.5-flash (fastest, cheapest)
#   Power:   gemini-1.5-pro (best reasoning)
#
# âœ“ GROQ MODELS (https://console.groq.com/docs/models)
#   Fast LLaMA: llama-3.3-70b-versatile, llama-3.1-8b-instant
#   Mixedmodal: mixtral-8x7b-32768
#
# âœ“ OLLAMA MODELS (https://ollama.ai/library)
#   Recommended: llama3.1:8b (balanced, good quality, 8GB RAM)
#   Lightweight: mistral, neural-chat, phi3 (faster, less memory)
#   Powerful: llama3.1:70b, llama3.2:70b (best quality, requires 40GB+ RAM)
# Type: string
# Default: llama3.1:8b
# ENV: LLM_MODEL
# CLI: --model <name>, -m <name>
# =============================================================================
LLM_MODEL=llama3.1:8b

# Custom API base URL for LLM provider (required for Ollama)
# Use this to:
#   - Connect to local Ollama: http://localhost:11434
#   - Use OpenAI-compatible proxies/self-hosted endpoints
#   - Route through a gateway or load balancer
#   - Use alternative OpenAI-compatible APIs (LM Studio, vLLM, etc.)
# Type: URL string
# Default: http://localhost:11434 (when using Ollama)
# ENV: LLM_API_BASE
# CLI: --llm-api-base <url>
# Note: Make sure Ollama is running: ollama serve
# =============================================================================
LLM_API_BASE=http://localhost:11434

# =============================================================================
# 2. EMBEDDING PROVIDER & MODEL CONFIGURATION
# =============================================================================
# Choose your embedding provider. Supported options:
#   - openai   : OpenAI embeddings (requires OPENAI_API_KEY)
#   - google   : Google embeddings (requires GOOGLE_API_KEY)
#   - ollama   : Local embeddings via Ollama (requires EMB_API_BASE)
# Default: ollama
# Note: Embeddings are used for document retrieval in RAG pipelines
# Recommendation: Use same provider as LLM when possible for consistency
# Type: string
# ENV: EMB_PROVIDER
# CLI: --emb-provider <name>
# =============================================================================
EMB_PROVIDER=ollama

# Specific embedding model to use with the provider above.
# Recommendation: Start with nomic-embed-text for Ollama (efficient, high quality)
#
# âœ“ OPENAI EMBEDDING MODELS (https://platform.openai.com/docs/models/embeddings)
#   Default: text-embedding-3-small (fast, cheap, good quality)
#   Premium: text-embedding-3-large (best quality, slower, more expensive)
#   Legacy:  text-embedding-ada-002
#
# âœ“ GOOGLE EMBEDDING MODELS (https://ai.google.dev/models)
#   Latest: models/text-embedding-004
#   Legacy: models/embedding-001
#
# âœ“ OLLAMA EMBEDDING MODELS (https://ollama.ai/library)
#   Recommended: nomic-embed-text (excellent quality, fast)
#   Lightweight: all-minilm (fastest, smaller)
#   Powerful: mxbai-embed-large (best quality, slower)
# Type: string
# Default: nomic-embed-text
# ENV: EMB_MODEL
# CLI: --emb-model <name>
# =============================================================================
EMB_MODEL=nomic-embed-text

# Custom API base URL for embedding provider (required for Ollama)
# Use this to:
#   - Connect to local Ollama: http://localhost:11434
#   - Use OpenAI-compatible embedding services
#   - Route through a gateway
# Type: URL string
# Default: http://localhost:11434 (when using Ollama)
# ENV: EMB_API_BASE
# CLI: --emb-api-base <url>
# Note: Must be the same as LLM_API_BASE when using Ollama
# =============================================================================
EMB_API_BASE=http://localhost:11434

# =============================================================================
# 3. API KEYS & AUTHENTICATION (OPTIONAL - Only for cloud providers)
# =============================================================================
# Only fill in the API key(s) for the provider(s) you're using.
# With Ollama (default), NO API KEYS are required - it runs locally!
# SECURITY: Never commit these to version control!
#
# âœ“ Get OpenAI API Key (OPTIONAL - only if not using Ollama)
#   1. Visit: https://platform.openai.com/api-keys
#   2. Sign up or log in to OpenAI account
#   3. Create new API key
#   4. Copy and paste below (starts with "sk-")
#   5. Keep it secret - regenerate if accidentally exposed
# Type: string (secret)
# Format: sk-... (OpenAI format)
# Required when: LLM_PROVIDER=openai or EMB_PROVIDER=openai
# ENV: OPENAI_API_KEY
# CLI: --openai-key <key>
# =============================================================================
# OPENAI_API_KEY=

# âœ“ Get Google API Key (for Gemini models) - OPTIONAL
#   1. Visit: https://aistudio.google.com/app/apikey
#   2. Click "Create API Key" â†’ "Create API key in new project"
#   3. Copy and paste below
#   4. Enable required APIs in Google Cloud Console
# Type: string (secret)
# Required when: LLM_PROVIDER=google or EMB_PROVIDER=google
# ENV: GOOGLE_API_KEY
# CLI: --google-key <key>
# =============================================================================
# GOOGLE_API_KEY=

# âœ“ Get Groq API Key - OPTIONAL
#   1. Visit: https://console.groq.com/keys
#   2. Sign up or log in to Groq account
#   3. Create new API key
#   4. Copy and paste below (starts with "gsk_")
# Type: string (secret)
# Format: gsk_... (Groq format)
# Required when: LLM_PROVIDER=groq
# Note: Groq only supports LLM, not embeddings. Use another provider for embeddings.
# ENV: GROQ_API_KEY
# CLI: --groq-key <key>
# =============================================================================
# GROQ_API_KEY=

# =============================================================================
# 4. RAG (RETRIEVAL-AUGMENTED GENERATION) STRATEGY
# =============================================================================
# Choose the RAG mode that best fits your use case:
#
# Strategy: basic (DEFAULT - Recommended for most cases)
#   - Simple retrieve-and-generate pipeline
#   - Fast and reliable
#   - Best for: General Q&A, straightforward knowledge bases
#   - Performance: âš¡âš¡âš¡ (Fastest)
#   - Quality: â­â­â­ (Good)
#
# Strategy: corrective
#   - Includes relevance checking of retrieved documents
#   - Automatically refines retrieval if confidence is low
#   - Best for: High-accuracy requirements, sensitive domains
#   - Performance: âš¡âš¡ (Moderate)
#   - Quality: â­â­â­â­ (Better)
#
# Strategy: adaptive
#   - Routes queries to different processing paths
#   - Adapts based on query complexity and document relevance
#   - Best for: Complex, heterogeneous knowledge bases
#   - Performance: âš¡ (Slower)
#   - Quality: â­â­â­â­ (Better)
#
# Strategy: multi_agent
#   - Uses specialized agents for different aspects
#   - Handles complex multi-step reasoning
#   - Best for: Complex reasoning, multi-topic queries
#   - Performance: ğŸŒ (Slowest)
#   - Quality: â­â­â­â­â­ (Best)
#
# Type: string (basic|corrective|adaptive|multi_agent)
# Default: basic
# ENV: RAG_MODE
# CLI: --rag-mode <mode>
# =============================================================================
RAG_MODE=basic

# =============================================================================
# 5. DOCUMENT PROCESSING CONFIGURATION
# =============================================================================
# Chunking strategy for splitting documents into retrievable pieces:
#
# Strategy: recursive (DEFAULT - Recommended)
#   - Uses character-level recursive splitting with overlap
#   - Maintains semantic boundaries where possible
#   - Fast and memory-efficient
#   - Best for: Most use cases, especially when performance matters
#   - Speed: âš¡âš¡âš¡ (Fastest)
#
# Strategy: semantic
#   - Uses semantic similarity to find natural breakpoints
#   - Preserves meaning across chunk boundaries
#   - Slower but produces higher-quality chunks
#   - Best for: High-precision retrieval, sensitive content
#   - Speed: ğŸŒ (Slower, ~5-10x)
#   - Quality: â­â­â­â­â­ (Better semantic coherence)
#
# Type: string (recursive|semantic)
# Default: recursive
# ENV: CHUNKING_STRATEGY
# CLI: --chunking-strategy <strategy>
# =============================================================================
CHUNKING_STRATEGY=recursive

# Enable Docling for advanced document parsing
# Docling provides sophisticated document structure extraction for:
#   - PDF with complex layouts, tables, figures
#   - DOCX/PPTX with formatting preservation
#   - HTML with semantic structure
#
# Benefits:
#   âœ“ Better table extraction and understanding
#   âœ“ Preserves document structure and hierarchy
#   âœ“ Extracts metadata and embedded images
#   âœ“ Handles complex PDFs with multiple columns
#
# Trade-offs:
#   - Requires additional dependencies: pip install docling
#   - Slower processing than standard parsing
#   - More memory intensive
#   - Only activate if you have complex documents
#
# Type: boolean (true|false)
# Default: false
# ENV: USE_DOCLING
# CLI: --use-docling true
# =============================================================================
USE_DOCLING=false

# =============================================================================
# 6. PERSISTENCE & CONVERSATION MEMORY
# =============================================================================
# Enable persistent memory for conversation continuity
#
# When enabled (true):
#   âœ“ Conversations are saved to database
#   âœ“ Threads can be resumed later
#   âœ“ Full conversation history is maintained
#   âœ“ Metrics and analytics are collected
#
# When disabled (false):
#   âœ“ Lower memory footprint
#   âœ“ Faster inference (no database writes)
#   âœ“ Better for stateless/serverless deployments
#   âœ— Conversations cannot be resumed
#   âœ— No conversation history or metrics
#
# Type: boolean (true|false)
# Default: true
# ENV: USE_PERSISTENT_MEMORY
# CLI: --use-persistent-memory true|false
# =============================================================================
USE_PERSISTENT_MEMORY=true

# Path to SQLite database for storing conversation checkpoints
#
# This database stores:
#   - Conversation threads and state
#   - Message history
#   - Conversation metadata (timestamps, participants, etc.)
#   - Recovery points for resuming conversations
#
# Database format: SQLite 3
# Auto-created: Yes (created on first use if doesn't exist)
# Storage: Local filesystem
# Backups: Manual (recommended to backup regularly)
#
# Examples:
#   - Relative path: knowledges/checkpoints.db
#   - Absolute path: /var/lib/doc-bases/checkpoints.db
#   - Network path: /mnt/shared/checkpoints.db
#
# Type: file path (string)
# Default: knowledges/checkpoints.db
# ENV: CHECKPOINT_DB_PATH
# CLI: --checkpoint-db-path <path>
# Only used when: USE_PERSISTENT_MEMORY=true
# =============================================================================
# CHECKPOINT_DB_PATH=knowledges/checkpoints.db

# Path to SQLite database for storing metrics and analytics
#
# This database stores:
#   - Query performance metrics (latency, tokens used, etc.)
#   - Model invocation statistics
#   - Retrieval quality metrics (hit rates, relevance scores)
#   - Embedding generation metrics
#   - Error and failure tracking
#
# Database format: SQLite 3
# Auto-created: Yes (created on first use if doesn't exist)
# Storage: Local filesystem
# Backups: Manual (recommended to backup regularly)
#
# Use cases:
#   - Performance monitoring and optimization
#   - Cost tracking (tokens, API calls)
#   - Quality assurance and debugging
#   - Usage analytics and reporting
#
# Type: file path (string)
# Default: knowledges/metrics.db
# ENV: METRICS_DB_PATH
# CLI: --metrics-db-path <path>
# =============================================================================
# METRICS_DB_PATH=knowledges/metrics.db

# =============================================================================
# 7. OBSERVABILITY & DEBUGGING - LANGSMITH
# =============================================================================
# LangSmith is an observability platform for LLM applications
# Features: Tracing, debugging, testing, monitoring
# Website: https://smith.langchain.com/
#
# Benefits of enabling:
#   âœ“ See detailed execution traces of LLM calls
#   âœ“ Debug retrieval and generation steps
#   âœ“ Monitor token usage and costs
#   âœ“ Test and evaluate prompt performance
#   âœ“ Collaborate with team on traces
#   âœ“ Performance profiling and optimization insights
#
# When to enable:
#   - During development and debugging
#   - When optimizing RAG pipeline performance
#   - For production monitoring and alerting
#   - When troubleshooting retrieval issues
#
# When to disable:
#   - Local development (when you don't need traces)
#   - To reduce API latency (minimal but adds ~50-100ms)
#   - To avoid additional service dependencies
#
# Type: boolean (true|false)
# Default: false
# ENV: LANGSMITH_TRACING
# CLI: --langsmith-tracing true|false
# =============================================================================
LANGSMITH_TRACING=false

# LangSmith API Key for authentication
#
# How to get your API key:
#   1. Visit: https://smith.langchain.com/
#   2. Sign in or create account
#   3. Go to Settings â†’ API Keys
#   4. Create new API key
#   5. Copy and paste below
#   6. Keep it secret - regenerate if accidentally exposed
#
# Format: API key usually starts with "ls_"
# Visibility: Keep this secret, never commit to version control
#
# Type: string (secret)
# Required when: LANGSMITH_TRACING=true
# ENV: LANGSMITH_API_KEY
# CLI: --langsmith-key <key>
# =============================================================================
LANGSMITH_API_KEY=

# LangSmith project name for organizing traces
#
# Purpose:
#   - Organize traces by project/environment
#   - Separate production, staging, development traces
#   - Team collaboration on specific projects
#
# Examples:
#   - doc-bases-production
#   - doc-bases-staging
#   - doc-bases-dev-[your-name]
#   - doc-bases-experiment-[name]
#
# Auto-created: Yes (LangSmith creates project if it doesn't exist)
#
# Type: string
# Default: doc-bases
# ENV: LANGSMITH_PROJECT
# CLI: --langsmith-project <name>
# Only used when: LANGSMITH_TRACING=true
# =============================================================================
LANGSMITH_PROJECT=doc-bases

# =============================================================================
# 8. QUICK START CONFIGURATION EXAMPLES
# =============================================================================
# Copy and uncomment ONE of the examples below to get started quickly.
# Each example represents a complete, working configuration.
# DEFAULT SETUP (below) is already configured for Ollama - just install it!

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 0: OLLAMA (DEFAULT - NO SETUP NEEDED!)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Getting started quickly, no API costs, local/private
# Cost: FREE (runs locally on your machine)
# Setup time: 2 minutes (install Ollama + pull models)
# Performance: Depends on hardware (GPU recommended)
# THIS IS THE DEFAULT - NO CHANGES NEEDED!
#
# To use:
#   1. Install Ollama: https://ollama.ai
#   2. Start Ollama in terminal: ollama serve
#   3. In another terminal, pull models:
#      ollama pull llama3.1:8b
#      ollama pull nomic-embed-text
#   4. Done! Just run your application
#
# Note: First request will be slow (model loading). Subsequent requests fast.
#
# Already configured above:
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:8b
# LLM_API_BASE=http://localhost:11434
# EMB_PROVIDER=ollama
# EMB_MODEL=nomic-embed-text
# EMB_API_BASE=http://localhost:11434

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 1: OPENAI (CLOUD-BASED)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Production use, reliable performance
# Cost: ~$0.02-0.15 per 1K tokens for gpt-4o-mini
# Setup time: 2 minutes (just need OpenAI API key)
# Performance: Fast and reliable
#
# To use:
#   1. Get API key: https://platform.openai.com/api-keys
#   2. Uncomment the lines below
#   3. Replace "sk-your-key-here" with your actual API key
#
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# LLM_API_BASE=
# EMB_PROVIDER=openai
# EMB_MODEL=text-embedding-3-small
# EMB_API_BASE=
# OPENAI_API_KEY=sk-your-key-here
# RAG_MODE=basic
# CHUNKING_STRATEGY=recursive

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 2: GOOGLE GEMINI (COST-EFFECTIVE CLOUD)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Budget-conscious, cost optimization
# Cost: Free tier available, ~$0.075 per 1M input tokens (flash model)
# Setup time: 2 minutes (just need Google API key)
# Performance: Fast (flash) or high-quality (pro)
#
# To use:
#   1. Get API key: https://aistudio.google.com/app/apikey
#   2. Uncomment the lines below
#   3. Replace "your-key-here" with your actual API key
#
# LLM_PROVIDER=google
# LLM_MODEL=gemini-1.5-flash
# LLM_API_BASE=
# EMB_PROVIDER=google
# EMB_MODEL=models/text-embedding-004
# EMB_API_BASE=
# GOOGLE_API_KEY=your-key-here
# RAG_MODE=basic
# CHUNKING_STRATEGY=recursive

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 3: GROQ (FASTEST CLOUD)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Speed-critical applications, low-latency requirements
# Cost: Competitive pricing, free tier available
# Setup time: 3 minutes (need Groq + OpenAI for embeddings)
# Performance: âš¡ Fastest LLM responses (often <1 second)
#
# To use:
#   1. Get Groq key: https://console.groq.com/keys
#   2. Get OpenAI key: https://platform.openai.com/api-keys (for embeddings)
#   3. Uncomment the lines below
#   4. Replace placeholders with your actual API keys
#
# Note: Groq doesn't provide embeddings, so we use OpenAI for embeddings
#
# LLM_PROVIDER=groq
# LLM_MODEL=llama-3.3-70b-versatile
# LLM_API_BASE=
# EMB_PROVIDER=openai
# EMB_MODEL=text-embedding-3-small
# EMB_API_BASE=
# GROQ_API_KEY=gsk_your-key-here
# OPENAI_API_KEY=sk-your-key-here
# RAG_MODE=basic
# CHUNKING_STRATEGY=recursive

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 4: OLLAMA WITH LARGER MODEL
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Higher quality responses (requires more RAM)
# Cost: FREE (runs locally on your machine)
# Performance: Slower but better quality
# Requirements: 70GB+ RAM
#
# To use:
#   1. Install Ollama: https://ollama.ai
#   2. Start Ollama: ollama serve
#   3. Pull larger model: ollama pull llama3.1:70b
#   4. Uncomment the lines below
#
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:70b
# LLM_API_BASE=http://localhost:11434
# EMB_PROVIDER=ollama
# EMB_MODEL=nomic-embed-text
# EMB_API_BASE=http://localhost:11434
# RAG_MODE=adaptive
# CHUNKING_STRATEGY=semantic

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 5: HYBRID (BEST OF BOTH WORLDS)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Power users wanting speed + quality
# Use Case: Fast LLM (Groq) + Semantic search (OpenAI)
# Cost: Moderate (Groq free tier + OpenAI for embeddings)
#
# LLM_PROVIDER=groq
# LLM_MODEL=llama-3.3-70b-versatile
# EMB_PROVIDER=openai
# EMB_MODEL=text-embedding-3-large
# GROQ_API_KEY=gsk_your-key-here
# OPENAI_API_KEY=sk-your-key-here
# RAG_MODE=adaptive
# CHUNKING_STRATEGY=semantic

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# EXAMPLE 6: ADVANCED RAG WITH MONITORING
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Best for: Production deployments with observability
# Features: Tracing, metrics, advanced RAG, semantic chunking
#
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# EMB_PROVIDER=openai
# EMB_MODEL=text-embedding-3-large
# OPENAI_API_KEY=sk-your-key-here
# RAG_MODE=corrective
# CHUNKING_STRATEGY=semantic
# USE_PERSISTENT_MEMORY=true
# CHECKPOINT_DB_PATH=knowledges/checkpoints.db
# METRICS_DB_PATH=knowledges/metrics.db
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=ls_your-key-here
# LANGSMITH_PROJECT=doc-bases-production

# =============================================================================
# TROUBLESHOOTING & SUPPORT
# =============================================================================
# Having issues? Check these resources:
#
# âœ“ Connection issues:
#   - Ollama: Verify "ollama serve" is running on http://localhost:11434
#   - Proxy: Check LLM_API_BASE and EMB_API_BASE URLs
#
# âœ“ API Key errors:
#   - Verify API keys are valid and not expired
#   - Check you have quota/credits with the provider
#   - Regenerate key if accidentally exposed
#
# âœ“ Performance issues:
#   - Use RAG_MODE=basic for fastest responses
#   - Use CHUNKING_STRATEGY=recursive (not semantic)
#   - Use smaller/faster models (gpt-4o-mini, gemini-1.5-flash)
#
# âœ“ Quality issues:
#   - Try RAG_MODE=corrective or adaptive
#   - Use CHUNKING_STRATEGY=semantic
#   - Use larger models (gpt-4o, gemini-1.5-pro)
#   - Enable USE_DOCLING=true for complex documents
#
# Documentation: https://github.com/yourusername/doc-bases
# Issues: https://github.com/yourusername/doc-bases/issues
# =============================================================================
